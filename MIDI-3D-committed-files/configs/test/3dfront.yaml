name: image2scene
tag: "test-3dfront"
exp_root_dir: "outputs"
seed: 42

data_cls: "midi.data.multi_object.MultiObjectDataModule"
data:
  scene_list: "data/3d-front/midi_room_ids.json"
  object_list: "data/3d-front/midi_furniture_ids.json"

  # Surface
  surface_root_dir: "data/3d-front/3D-FRONT-SURFACE"
  surface_suffix: "npy"
  num_surface_samples_per_object: 20480

  max_num_instances: 7
  padding: false # true for training
  num_instances_per_batch: null

  # Image input
  image_root_dir: "data/3d-front/3D-FRONT-RENDER"
  image_prefix: ["render", "rerender"]
  image_suffix: "webp"
  idmap_prefix: "semantic"
  idmap_suffix: "png"
  background_color: "white"
  image_names: ["0000", "0001", "0002", "0003", "0004", "0005", "0006", "0007", "0008", "0009", "0010"]
  height: 512
  width: 512

  use_scene_image: true
  remove_scene_bg: false

  # Data processing
  skip_small_object: true
  small_image_proportion: 0.001

  ## Mask perturbation
  morph_perturb: false

  # Data augmentation -> mix objaverse
  do_mix: false

  train_indices: [0, -1000]
  val_indices: [-1010, -990]
  test_indices: [-1000, null]

  batch_size: 1
  eval_batch_size: 1

  num_workers: 16

system_cls: midi.systems.system_midi.MIDISystem
system:
  check_train_every_n_steps: 1000
  cleanup_after_validation_step: true
  cleanup_after_test_step: true

  # Model / Adapter
  pretrained_model_name_or_path: "pretrained_weights/MIDI-3D"

  ## Attention processor
  set_self_attn_module_names: ["blocks.8", "blocks.9", "blocks.10", "blocks.11", "blocks.12"]

  # Training
  image_drop_prob: 0.1
  new_cond_size: 512

  vae_slicing_length: 5
  gradient_checkpointing: true

  # Noise sampler
  weighting_scheme: logit_normal_dist

  # Evaluation
  eval_seed: 42
  eval_num_inference_steps: 50
  eval_guidance_scale: 7.0

  # Others
  visual_resolution: ${data.height}

  # optimizer definition
  # you can set different learning rates separately for each group of parameters, but note that if you do this you should specify EVERY trainable parameters
  optimizer:
    name: AdamW
    args:
      lr: 5e-5
      betas: [0.9, 0.999]
      weight_decay: 0.01

  scheduler:
    name: ConstantLR
    interval: step
    args:
      factor: 1.0
      total_iters: 9999999
    milestones: [2000]

trainer:
  max_epochs: 10
  log_every_n_steps: 10
  num_sanity_val_steps: 1
  check_val_every_n_epoch: 1
  enable_progress_bar: true
  precision: bf16-mixed
  gradient_clip_val: 1.0
  strategy: ddp
  accumulate_grad_batches: 1

checkpoint:
  save_last: true # whether to save at each validation time
  save_top_k: -1
  every_n_epochs: 1 # do not save at all for debug purpose